{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa2b7a7-b124-43b4-af52-223d613c58b7",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "### Problem 6.\n",
    "For this problem, you will use each of the three models you’ve constructed in problems 2–4 to\n",
    "evaluate the probability of a toy corpus of sentences (containing just the first two sentences of the\n",
    "Brown corpus) found at: toy_corpus.txt\n",
    "\n",
    "The sentences are contained in this corpus in the same format as the other corpora you’ve been\n",
    "using so far: one sentence per line and words separated by a space. As before, you’ll need to remove\n",
    "the end-of-line character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "42b215c8-814c-4704-aab5-cbb6b69d1051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004051863857374392\n",
      "0.003646677471636953\n"
     ]
    }
   ],
   "source": [
    "# Problem 2,  wouter's version\n",
    "##\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "from nltk import pos_tag\n",
    "\n",
    "vocab = open(\"brown_vocab_100.txt\")\n",
    "\n",
    "#load the indices dictionary\n",
    "word_index_dict = {}\n",
    "for i, line in enumerate(vocab):\n",
    "    #import part 1 code to build dictionary\n",
    "    word_index_dict[line.rstrip()] = i\n",
    "\n",
    "f = open(\"brown_100.txt\")\n",
    "\n",
    "counts = np.zeros(len(word_index_dict)) #initialize counts to a zero vector\n",
    "\n",
    "#iterate through file and update counts\n",
    "for sent in f:\n",
    "    sent_list = sent.lower().split()\n",
    "\n",
    "    for word in sent_list:\n",
    "        ind = word_index_dict[word]\n",
    "        counts[ind] += 1\n",
    "\n",
    "f.close()\n",
    "\n",
    "#normalize and writeout counts. \n",
    "\n",
    "#print(counts) edited for space\n",
    "probs = counts / np.sum(counts)\n",
    "print(probs[0])\n",
    "print(probs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c87fa-a263-4e00-8f8b-1b3462bc8576",
   "metadata": {},
   "source": [
    "#### *Beginning of Problem 6*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fdc773f7-d78c-407f-8e82-45b858f51310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint probability of the 2nd sentence:  4.840083877820268e-99\n",
      "sent_len:  45\n",
      "sentprob:  4.840083877820268e-99\n",
      "perplexity of the 2nd sentence:  153.03157461392973\n"
     ]
    }
   ],
   "source": [
    "#open and rea dthe toy_corpus.txt\n",
    "toy_corpus_file = open(\"toy_corpus.txt\", \"r\")\n",
    "toy_corpus_text = toy_corpus_file.read()\n",
    "toy_corpus_file.close()\n",
    "sentences = toy_corpus_text.split('\\n')\n",
    "\n",
    "#open to write the probabilities into the new txt file\n",
    "output_file = open(\"unigram_eval.txt\", \"w\")\n",
    "\n",
    "#iterate thru sentences and calculate joint prob of each one\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    #initialize sentence prob\n",
    "    sentprob = 1\n",
    "    #computes the joint prob per word in sentence\n",
    "    for word in words:\n",
    "        #finds index of word\n",
    "        index = word_index_dict.get(word.lower(), None)\n",
    "        if index is not None:\n",
    "            #finds prob of word\n",
    "            wordprob = probs[index]\n",
    "            #updated sentence prob by * with word prob\n",
    "            sentprob *= wordprob\n",
    "    #To calculate the perplexity, first calculate the length of the sentence in words (be sure to\n",
    "    #include the end-of-sentence word)   \n",
    "    sent_len =len(words) - words.count(\"</s>\")  + 1\n",
    "    #calculate perplexity\n",
    "    perplexity = 1/(pow(sentprob, 1.0/sent_len))    \n",
    "    \n",
    "    #added the perplexity check to unigram_eval.txt (instead of joint prob, to check joint, have it write\n",
    "    #output_file.write(str(sentprob) + '\\n') instead\n",
    "    output_file.write(str(perplexity) + '\\n')\n",
    "\n",
    "#close file\n",
    "output_file.close()\n",
    "\n",
    "#verify jopint pro b is 4.84008387782\n",
    "print('Joint probability of the 2nd sentence: ', sentprob)\n",
    "\n",
    "print('sent_len: ', sent_len)\n",
    "print('sentprob: ', sentprob)\n",
    "#cheeck perplexity of 2nd sentence is 153\n",
    "print('perplexity of the 2nd sentence: ', perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed1c8f-c185-4f08-9b70-f9226d856a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7221fd-df36-428d-ac43-c9587077f9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7143bf0-f8bc-428f-b074-0a60bc540630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
