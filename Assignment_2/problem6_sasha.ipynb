{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa2b7a7-b124-43b4-af52-223d613c58b7",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "### Problem 6.\n",
    "For this problem, you will use each of the three models you’ve constructed in problems 2–4 to\n",
    "evaluate the probability of a toy corpus of sentences (containing just the first two sentences of the\n",
    "Brown corpus) found at: toy_corpus.txt\n",
    "\n",
    "The sentences are contained in this corpus in the same format as the other corpora you’ve been\n",
    "using so far: one sentence per line and words separated by a space. As before, you’ll need to remove\n",
    "the end-of-line character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "42b215c8-814c-4704-aab5-cbb6b69d1051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004051863857374392\n",
      "0.003646677471636953\n"
     ]
    }
   ],
   "source": [
    "# Problem 2,  wouter's version\n",
    "##\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "from nltk import pos_tag\n",
    "\n",
    "vocab = open(\"brown_vocab_100.txt\")\n",
    "\n",
    "#load the indices dictionary\n",
    "word_index_dict = {}\n",
    "for i, line in enumerate(vocab):\n",
    "    #import part 1 code to build dictionary\n",
    "    word_index_dict[line.rstrip()] = i\n",
    "\n",
    "f = open(\"brown_100.txt\")\n",
    "\n",
    "counts = np.zeros(len(word_index_dict)) #initialize counts to a zero vector\n",
    "\n",
    "#iterate through file and update counts\n",
    "for sent in f:\n",
    "    sent_list = sent.lower().split()\n",
    "\n",
    "    for word in sent_list:\n",
    "        ind = word_index_dict[word]\n",
    "        counts[ind] += 1\n",
    "\n",
    "f.close()\n",
    "\n",
    "#normalize and writeout counts. \n",
    "\n",
    "#print(counts) edited for space\n",
    "probs = counts / np.sum(counts)\n",
    "print(probs[0])\n",
    "print(probs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c87fa-a263-4e00-8f8b-1b3462bc8576",
   "metadata": {},
   "source": [
    "#### *Beginning of Problem 6*\n",
    "First, you’ll edit problem2.py, and add code at the bottom of the script to iterate through each\n",
    "sentence in the toy corpus and calculate the joint probability of all the words in the sentence under\n",
    "the unigram model. Then write the probability of each sentence to a file unigram_eval.txt,\n",
    "formatted to have one probability for each line of the output file.\n",
    "\n",
    "To do this, you’ll be writing a loop within a loop very similarly to how you iterated through the\n",
    "training corpus to estimate the model parameters. But instead of incrementing counts after each\n",
    "word, you’ll be updating the joint probability of the sentence (multiplying the probabilities of each\n",
    "word together). One easy way to do this is to initialize sentprob = 1 prior to looping through\n",
    "the words in the sentence, and then just update sentprob *= wordprob with the probability\n",
    "of each word. At the end of the loop, sentprob will contain the total joint probability of the whole\n",
    "sentence. To verify that this worked correctly, note that the joint probability of the second sentence\n",
    "under this model should be 4.84008387782e-99\n",
    "\n",
    "Next, you’ll transform this joint probability into a perplexity (of each sentence), and write that to the\n",
    "file instead. To calculate the perplexity, first calculate the length of the sentence in words (be sure to\n",
    "include the end-of-sentence word) and store that in a variable sent_len, and then you can\n",
    "calculate perplexity = 1/(pow(sentprob, 1.0/sent_len)), which reproduces the\n",
    "definition of perplexity we discussed in class.\n",
    "\n",
    "Now, write the perplexity of each sentence to the output file instead of the joint probabilities. To\n",
    "verify that you’ve done this correctly, note that the perplexity of the second sentence with this\n",
    "model should be about 153.\n",
    "\n",
    "### MLE unigram model (wouter version) (problem 2):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fdc773f7-d78c-407f-8e82-45b858f51310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint probability of the 2nd sentence:  4.840083877820268e-99\n",
      "sent_len:  45\n",
      "sentprob:  4.840083877820268e-99\n",
      "perplexity of the 2nd sentence:  153.03157461392973\n"
     ]
    }
   ],
   "source": [
    "#open and rea dthe toy_corpus.txt\n",
    "toy_corpus_file = open(\"toy_corpus.txt\", \"r\")\n",
    "toy_corpus_text = toy_corpus_file.read()\n",
    "toy_corpus_file.close()\n",
    "sentences = toy_corpus_text.split('\\n')\n",
    "\n",
    "#open to write the probabilities into the new txt file\n",
    "output_file = open(\"unigram_eval.txt\", \"w\")\n",
    "\n",
    "#iterate thru sentences and calculate joint prob of each one\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    #initialize sentence prob\n",
    "    sentprob = 1\n",
    "    #computes the joint prob per word in sentence\n",
    "    for word in words:\n",
    "        #finds index of word\n",
    "        index = word_index_dict.get(word.lower(), None)\n",
    "        if index is not None:\n",
    "            #finds prob of word\n",
    "            wordprob = probs[index]\n",
    "            #updated sentence prob by * with word prob\n",
    "            sentprob *= wordprob\n",
    "    #To calculate the perplexity, first calculate the length of the sentence in words (be sure to\n",
    "    #include the end-of-sentence word)  \n",
    "    #not sure why the perplexity is off without counting the <s>\n",
    "    sent_len =len(words) - words.count(\"<s>\")  + 1\n",
    "    #calculate perplexity\n",
    "    perplexity = 1/(pow(sentprob, 1.0/sent_len))    \n",
    "    \n",
    "    #added the perplexity check to unigram_eval.txt (instead of joint prob, to check joint, have it write\n",
    "    #output_file.write(str(sentprob) + '\\n') instead\n",
    "    output_file.write(str(perplexity) + '\\n')\n",
    "\n",
    "#close file\n",
    "output_file.close()\n",
    "\n",
    "#verify jopint pro b is 4.84008387782\n",
    "print('Joint probability of the 2nd sentence: ', sentprob)\n",
    "\n",
    "print('sent_len: ', sent_len)\n",
    "print('sentprob: ', sentprob)\n",
    "#cheeck perplexity of 2nd sentence is 153\n",
    "print('perplexity of the 2nd sentence: ', perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121c6af9-9c8d-4443-8a11-1d5400c25a83",
   "metadata": {},
   "source": [
    "### MLE bigram model: (wouter version) (problem 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "30cbb43d-59a3-4aeb-aed2-f89f27308441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.08333333333333333, 0.00641025641025641, 0.3333333333333333]\n",
      "sent_len:  46\n",
      "sentprob:  2.0462234274543635e-41\n",
      "perplexity of the 2nd sentence for MLE bigram is:  7.6655707941796365\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "import codecs\n",
    "# Read the toy corpus\n",
    "toy_corpus_file = open(\"toy_corpus.txt\", \"r\")\n",
    "toy_corpus_text = toy_corpus_file.read()\n",
    "toy_corpus_file.close()\n",
    "sentences = toy_corpus_text.split('\\n')\n",
    "\n",
    "vocab = codecs.open(\"brown_vocab_100.txt\")\n",
    "\n",
    "#load the indices dictionary\n",
    "word_index_dict = {}\n",
    "for i, line in enumerate(vocab):\n",
    "    #import part 1 code to build dictionary\n",
    "    word_index_dict[line.rstrip()] = i\n",
    "\n",
    "f = codecs.open(\"brown_100.txt\")\n",
    "\n",
    "counts = np.zeros((len(word_index_dict), len(word_index_dict))) #initialize numpy 0s matrix\n",
    "\n",
    "\n",
    "#iterate through file and update counts\n",
    "prev_word = \"<s>\"\n",
    "\n",
    "for sent in f:\n",
    "    sent_list = sent.lower().split()\n",
    "    for word in sent_list:\n",
    "        next_word = word\n",
    "        ind_y = word_index_dict[next_word]\n",
    "        ind_x = word_index_dict[prev_word]\n",
    "        counts[ind_x, ind_y] += 1\n",
    "        prev_word = next_word      \n",
    "\n",
    "# normalize counts\n",
    "probs = normalize(counts, norm='l1', axis=1)\n",
    "\n",
    "#writeout bigram probabilities\n",
    "the_ind = word_index_dict[\"the\"]\n",
    "all_ind = word_index_dict[\"all\"]\n",
    "jury_ind = word_index_dict[\"jury\"]\n",
    "campaign_ind = word_index_dict[\"campaign\"]\n",
    "calls_ind = word_index_dict[\"calls\"]\n",
    "anonymous_ind = word_index_dict[\"anonymous\"]\n",
    "\n",
    "fourprobs = [probs[all_ind, the_ind],\n",
    "                probs[the_ind, jury_ind],\n",
    "                probs[the_ind, campaign_ind],\n",
    "                probs[anonymous_ind, calls_ind]]\n",
    "print(fourprobs)\n",
    "\n",
    "with open(\"bigram_probs.txt\", 'w') as file:\n",
    "    for p in fourprobs:\n",
    "        file.write(f\"{p}\\n\")\n",
    "f.close()\n",
    "\n",
    "#start and open the perplexity file\n",
    "output_file = open(\"bigram_eval.txt\", \"w\")\n",
    "\n",
    "#perplexities calculations for toy corpus\n",
    "#iterates through sentences\n",
    "for sent in sentences:\n",
    "    words = sent.lower().split()\n",
    "    sentprob = 1\n",
    "    prev_word = \"<s>\"\n",
    "    for word in words:\n",
    "        next_word = word\n",
    "        ind_y = word_index_dict[next_word]\n",
    "        ind_x = word_index_dict[prev_word]\n",
    "        if ind_x is not None and ind_y is not None:\n",
    "            wordprob = probs[ind_x, ind_y]\n",
    "            sentprob *= wordprob\n",
    "        prev_word = next_word\n",
    "    sent_len =len(words) + 1\n",
    "    #calculate perplexity\n",
    "    perplexity = 1/(pow(sentprob, (1.0/sent_len)))  \n",
    "    output_file.write(str(perplexity) + '\\n')\n",
    "\n",
    "#close file\n",
    "output_file.close()\n",
    "\n",
    "print('sent_len: ', sent_len)\n",
    "print('sentprob: ', sentprob)\n",
    "#cheeck perplexity of 2nd sentence is about 7.57 for the MLE bigram\n",
    "print('perplexity of the 2nd sentence for MLE bigram is: ', perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6100d31a-a840-46e4-862a-7d209a6be044",
   "metadata": {},
   "source": [
    "### MLE smoother model (wouter version) (problem 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2b7221fd-df36-428d-ac43-c9587077f9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01336573511543135, 0.05520438263801095, 0.004635482511588706, 0.013048635824436538]\n",
      "sent_len:  45\n",
      "sentprob:  2.949110312537236e-79\n",
      "perplexity of the 2nd sentence for alpha-smoothed model is:  55.60552908217579\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "import codecs\n",
    "# Read the toy corpus\n",
    "toy_corpus_file = open(\"toy_corpus.txt\", \"r\")\n",
    "toy_corpus_text = toy_corpus_file.read()\n",
    "toy_corpus_file.close()\n",
    "#sentences = toy_corpus_text.split('\\n')\n",
    "\n",
    "vocab = codecs.open(\"brown_vocab_100.txt\")\n",
    "\n",
    "#load the indices dictionary\n",
    "word_index_dict = {}\n",
    "for i, line in enumerate(vocab):\n",
    "    #import part 1 code to build dictionary\n",
    "    word_index_dict[line.rstrip()] = i\n",
    "\n",
    "f = codecs.open(\"brown_100.txt\")\n",
    "\n",
    "counts = np.zeros((len(word_index_dict), len(word_index_dict))) #initialize numpy 0s matrix\n",
    "\n",
    "#iterate through file and update counts\n",
    "prev_word = \"<s>\"\n",
    "\n",
    "for sent in f:\n",
    "    sent_list = sent.lower().split()\n",
    "    for word in sent_list:\n",
    "        next_word = word\n",
    "        ind_y = word_index_dict[next_word]\n",
    "        ind_x = word_index_dict[prev_word]\n",
    "        counts[ind_x, ind_y] += 1\n",
    "        prev_word = next_word\n",
    "\n",
    "counts += 0.1\n",
    "\n",
    "# normalize counts\n",
    "probs = normalize(counts, norm='l1', axis=1)\n",
    "\n",
    "#writeout bigram probabilities\n",
    "the_ind = word_index_dict[\"the\"]\n",
    "all_ind = word_index_dict[\"all\"]\n",
    "jury_ind = word_index_dict[\"jury\"]\n",
    "campaign_ind = word_index_dict[\"campaign\"]\n",
    "calls_ind = word_index_dict[\"calls\"]\n",
    "anonymous_ind = word_index_dict[\"anonymous\"]\n",
    "\n",
    "fourprobs = [probs[all_ind, the_ind],\n",
    "                probs[the_ind, jury_ind],\n",
    "                probs[the_ind, campaign_ind],\n",
    "                probs[anonymous_ind, calls_ind]]\n",
    "\n",
    "print(fourprobs)\n",
    "\n",
    "with open(\"smooth_probs.txt\", 'w') as file:\n",
    "    for p in fourprobs:\n",
    "        file.write(f\"{p}\\n\")\n",
    "\n",
    "f.close()\n",
    "\n",
    "\n",
    "#start and open the perplexity file\n",
    "output_file = open(\"smoothed_eval.txt\", \"w\")\n",
    "\n",
    "# Reopen toy corpus file to read from the beginning\n",
    "toy_corpus_file = open(\"toy_corpus.txt\", \"r\")\n",
    "\n",
    "#perplexities calculations for toy corpus\n",
    "#iterates through sentences\n",
    "for sent in toy_corpus_file:\n",
    "    words = sent.lower().split()\n",
    "    sentprob = 1\n",
    "    prev_word = \"<s>\"\n",
    "    for word in words:\n",
    "        next_word = word\n",
    "        ind_y = word_index_dict[next_word]\n",
    "        ind_x = word_index_dict[prev_word]\n",
    "        if ind_x is not None and ind_y is not None:\n",
    "            wordprob = probs[ind_x, ind_y]\n",
    "            sentprob *= wordprob\n",
    "        prev_word = next_word\n",
    "    # added back in the  - words.count(\"<s>\") as its closer in terms of perplexity, but cant figure out difference between the two problems why..\n",
    "    #sent_len is now 45, compared to 46 in the MLE bigram... and 45 in the unigram. \n",
    "    sent_len =len(words) - words.count(\"<s>\") + 1\n",
    "    #calculate perplexity\n",
    "    perplexity = 1/(pow(sentprob, (1.0/sent_len)))  \n",
    "    output_file.write(str(perplexity) + '\\n')\n",
    "\n",
    "#close file\n",
    "output_file.close()\n",
    "toy_corpus_file.close()\n",
    "\n",
    "print('sent_len: ', sent_len)\n",
    "print('sentprob: ', sentprob)\n",
    "#cheeck perplexity of 2nd sentence is about 54.28 for the alpha-smoothed model\n",
    "print('perplexity of the 2nd sentence for alpha-smoothed model is: ', perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7143bf0-f8bc-428f-b074-0a60bc540630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
