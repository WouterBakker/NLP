{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict, Counter, OrderedDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute a list of unique words sorted by descending frequency for:\n",
    "## (i) the whole corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts = Counter(brown.words())\n",
    "sorted_wordcounts = dict(wordcounts.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (ii) two different genres of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brown.categories()\n",
    "\n",
    "genre_adventure = brown.words(categories='adventure')   \n",
    "genre_news = brown.words(categories='news')   \n",
    "\n",
    "sorted_wordcounts_adventure = dict(Counter(genre_adventure).most_common())\n",
    "sorted_wordcounts_newss = dict(Counter(genre_news).most_common())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, extract the following information (should be visible in your code and output files): number of tokens; number of types; number of words; average number of words per sentence; average word length. You should also run a default part-of-speech tagger on the dataset and identify the ten most frequent POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 1161192\n",
      "Number of types: 56057\n",
      "Number of words: 1161192\n",
      "Average sentence length: 20.250994070456922\n",
      "Average sentence length: 4.276538246904905\n",
      "10 most common POS tags: [('NN', 152470), ('IN', 120557), ('AT', 97959), ('JJ', 64028), ('.', 60638), (',', 58156), ('NNS', 55110), ('CC', 37718), ('RB', 36464), ('NP', 34476)]\n"
     ]
    }
   ],
   "source": [
    "# The typeâ€“token distinction separates types (abstract descriptive concepts) from tokens (objects that instantiate concepts)\n",
    "# https://en.wikipedia.org/wiki/Type%E2%80%93token_distinction\n",
    "# Assuming the corpus has been tokenized by NLTK: \n",
    "print(f\"Number of tokens: {len(brown.words())}\")\n",
    "\n",
    "# The number of unique words then represent the types (?)\n",
    "print(f\"Number of types: {len(sorted_wordcounts)}\")\n",
    "\n",
    "print(f\"Number of words: {len(brown.words())}\")\n",
    "\n",
    "# Average len of sentences\n",
    "avg_len_sents = len(brown.words()) / len(brown.sents())\n",
    "print(f\"Average sentence length: {avg_len_sents}\")\n",
    "\n",
    "# Average word length\n",
    "avg_len_word = sum([len(x) for x in brown.words()])/len(brown.words())\n",
    "print(f\"Average sentence length: {avg_len_word}\")\n",
    "\n",
    "\n",
    "# 10 most frequent POS tags (run explicit tagger or can we use the provided tags?)\n",
    "\n",
    "dd = defaultdict(int)\n",
    "\n",
    "for x,y in brown.tagged_words():\n",
    "    dd[y] += 1\n",
    "    \n",
    "print(f\"10 most common POS tags: {Counter(dd).most_common()[:10]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
